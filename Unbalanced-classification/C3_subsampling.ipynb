{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0c375f",
   "metadata": {},
   "source": [
    "Submuestreo\n",
    "\n",
    "El submuestreo es una técnica que se utiliza para equilibrar las clases dentro de un conjunto de datos de entrenamiento. Al reducir la frecuencia de las observaciones de una clase predominante, podemos mejorar el rendimiento del modelo de aprendizaje automático al evitar que esté sesgado hacia la clase más común. ¿Cómo podemos hacer que las observaciones de una clase frecuente sean menos frecuentes en los datos?\n",
    "\n",
    "En lugar de repetir las preguntas importantes, también podemos eliminar una parte de las que no son importantes. Para ello, podemos utilizar la técnica de submuestreo.\n",
    "\n",
    "El submuestreo se realiza en varios pasos:\n",
    "\n",
    "División del conjunto de datos: separar el dataset de entrenamiento en dos grupos, uno para cada clase (positiva y negativa).\n",
    "\n",
    "Selección aleatoria: usa la función sample() para eliminar aleatoriamente una proporción de las observaciones de la clase predominante (negativa en este caso).\n",
    "Creación de una nueva muestra de entrenamiento: combina las observaciones restantes de la clase negativa con todas las observaciones de la clase positiva para forma un nuevo conjunto de entrenamiento.\n",
    "\n",
    "Mezcla de datos: si los datos no están bien mezclados, el modelo podría aprender patrones que no son realmente representativos del problema que se está abordando, sino que simplemente reflejan el orden en que los datos se presentaron durante el entrenamiento.\n",
    "\n",
    "Para eliminar aleatoriamente algunos elementos de la tabla, utiliza la función sample(). Esta función requiere un parámetro llamado frac ('fraction' o fracción), que especifica la proporción de los elementos totales que quieres retener. Por ejemplo, frac=0.1 significa que la función seleccionará aleatoriamente el 10% de los elementos de la tabla original para formar una nueva muestra.\n",
    "\n",
    "print(features_train.shape)\n",
    "\n",
    "features_sample = features_train.sample(frac=0.1, random_state=12345)\n",
    "\n",
    "print(features_sample.shape)\n",
    "\n",
    "(37995, 196)\n",
    "\n",
    "(3800, 196)\n",
    "\n",
    "Especifica random_state=12345 para que tu código sea más fácil de revisar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621205e",
   "metadata": {},
   "source": [
    "1. Para realizar el submuestreo, escribe una función downsample() y pásale tres argumentos:\n",
    "\n",
    "- features\n",
    "- target\n",
    "- fraction (fracción de observaciones negativas a mantener)\n",
    "\n",
    "La función debe realizar el submuestreo y devolver tanto las características modificadas como las etiquetas. Antes de devolver estos datos, asegúrate de mezclarlos para evitar sesgos en el proceso de aprendizaje del modelo.\n",
    "\n",
    "Llama a la función para los datos de entrenamiento con una fracción de 0.1. Muestra en la pantalla los tamaños de las muestras (en precódigo).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98269b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = pd.read_csv('/datasets/travel_insurance_us_preprocessed.csv')\n",
    "\n",
    "target = data['Claim']\n",
    "features = data.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345\n",
    ")\n",
    "\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    # < escribe el código aquí >\n",
    "    \n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones]\n",
    "    )\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones]\n",
    "    )\n",
    "    features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state = 12345)\n",
    "\n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "\n",
    "features_downsampled, target_downsampled = downsample(\n",
    "    features_train, target_train, 0.1\n",
    ")\n",
    "\n",
    "print(features_downsampled.shape)\n",
    "print(target_downsampled.shape)\n",
    "\n",
    "#(4325, 196)\n",
    "#(4325,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2bb282",
   "metadata": {},
   "source": [
    "Hemos reducido los datos a través de la fuerza de voluntad, el pensamiento positivo y el submuestreo. ¿Sientes que te estás moviendo sutilmente hacia el nirvana de la ciencia de datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f27d0d",
   "metadata": {},
   "source": [
    "2. Entrena el modelo LogisticRegression con los nuevos datos. Encuentra el valor F1 y muéstralo en la pantalla (en precódigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data = pd.read_csv('/datasets/travel_insurance_us_preprocessed.csv')\n",
    "\n",
    "target = data['Claim']\n",
    "features = data.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345\n",
    ")\n",
    "\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)]\n",
    "        + [features_ones]\n",
    "    )\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)]\n",
    "        + [target_ones]\n",
    "    )\n",
    "\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345\n",
    "    )\n",
    "\n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "\n",
    "features_downsampled, target_downsampled = downsample(\n",
    "    features_train, target_train, 0.1\n",
    ")\n",
    "\n",
    "# < escribe el código aquí >\n",
    "model = LogisticRegression(solver='liblinear', random_state=12345)\n",
    "model.fit(features_downsampled, target_downsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "#F1: 0.13333333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc96960",
   "metadata": {},
   "source": [
    "¡Sí funciona! La calidad ha aumentado. La mejora no es espectacular pero no te desanimes: los resultados pueden variar para diferentes conjuntos de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
